{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\install\\\\spark-3.5.3-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://aadith-ltp.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ReadDF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cf77dd1270>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54055)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shravanr\\AppData\\Local\\anaconda3\\envs\\Spark_3_5_3\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\shravanr\\AppData\\Local\\anaconda3\\envs\\Spark_3_5_3\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\shravanr\\AppData\\Local\\anaconda3\\envs\\Spark_3_5_3\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\shravanr\\AppData\\Local\\anaconda3\\envs\\Spark_3_5_3\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\install\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\install\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"C:\\install\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\install\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"c:\\Users\\shravanr\\AppData\\Local\\anaconda3\\envs\\Spark_3_5_3\\lib\\socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ReadDF\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .load(r\"C:\\Users\\shravanr\\learning\\spark\\pyspark\\input_dir\\data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(\"EmployeeId int,\tEmployeeName string, Department\tstring, Age int, Salary float\")\\\n",
    "    .load(r\"C:\\Users\\shravanr\\learning\\spark\\pyspark\\input_dir\\data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeId: integer (nullable = true)\n",
      " |-- EmployeeName: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Salary: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .schema(\"EmployeeId int,\tEmployeeName string, Department\tstring, Age int, Salary float\")\\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .load(r\"C:\\Users\\shravanr\\learning\\spark\\pyspark\\input_dir\\data_no_header.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------+---+-------+\n",
      "|EmployeeId|      EmployeeName| Department|Age| Salary|\n",
      "+----------+------------------+-----------+---+-------+\n",
      "|         1|          John Doe|Engineering| 30|75000.0|\n",
      "|         2|        Jane Smith|  Marketing| 28|65000.0|\n",
      "|         3|   Michael Johnson|      Sales| 35|70000.0|\n",
      "|         4|       Emily Davis|         HR| 40|60000.0|\n",
      "|         5|     William Brown|Engineering| 45|80000.0|\n",
      "|         6|     Olivia Wilson|  Marketing| 32|62000.0|\n",
      "|         7|       James Jones|      Sales| 38|71000.0|\n",
      "|         8|        Ava Garcia|         HR| 29|59000.0|\n",
      "|         9|   Benjamin Miller|Engineering| 33|77000.0|\n",
      "|        10|Charlotte Martinez|  Marketing| 31|63000.0|\n",
      "|        11|   Elijah Anderson|      Sales| 36|72000.0|\n",
      "|        12|     Sophia Thomas|         HR| 27|61000.0|\n",
      "|        13|      Lucas Taylor|Engineering| 34|76000.0|\n",
      "|        14|    Isabella Moore|  Marketing| 30|64000.0|\n",
      "|        15|     Mason Jackson|      Sales| 37|73000.0|\n",
      "|        16|        Mia Martin|         HR| 28|62000.0|\n",
      "|        17|         Ethan Lee|Engineering| 39|78000.0|\n",
      "|        18|      Amelia Perez|  Marketing| 29|65000.0|\n",
      "|        19|Alexander Thompson|      Sales| 41|74000.0|\n",
      "|        20|      Harper White|         HR| 30|63000.0|\n",
      "+----------+------------------+-----------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------+---+-------+\n",
      "|EmployeeId|      EmployeeName| Department|Age| Salary|\n",
      "+----------+------------------+-----------+---+-------+\n",
      "|         1|          John Doe|Engineering| 30|75000.0|\n",
      "|         2|        Jane Smith|  Marketing| 28|65000.0|\n",
      "|         3|   Michael Johnson|      Sales| 35|70000.0|\n",
      "|         4|       Emily Davis|         HR| 40|60000.0|\n",
      "|         5|     William Brown|Engineering| 45|80000.0|\n",
      "|         6|     Olivia Wilson|  Marketing| 32|62000.0|\n",
      "|         7|       James Jones|      Sales| 38|71000.0|\n",
      "|         8|        Ava Garcia|         HR| 29|59000.0|\n",
      "|         9|   Benjamin Miller|Engineering| 33|77000.0|\n",
      "|        10|Charlotte Martinez|  Marketing| 31|63000.0|\n",
      "|        11|   Elijah Anderson|      Sales| 36|72000.0|\n",
      "|        12|     Sophia Thomas|         HR| 27|61000.0|\n",
      "|        13|      Lucas Taylor|Engineering| 34|76000.0|\n",
      "|        14|    Isabella Moore|  Marketing| 30|64000.0|\n",
      "|        15|     Mason Jackson|      Sales| 37|73000.0|\n",
      "|        16|        Mia Martin|         HR| 28|62000.0|\n",
      "|        17|         Ethan Lee|Engineering| 39|78000.0|\n",
      "|        18|      Amelia Perez|  Marketing| 29|65000.0|\n",
      "|        19|Alexander Thompson|      Sales| 41|74000.0|\n",
      "|        20|      Harper White|         HR| 30|63000.0|\n",
      "+----------+------------------+-----------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .schema(\"EmployeeId int,\tEmployeeName string, Department\tstring, Age int, Salary float\")\\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"samplingRatio\", \"0.001\") \\\n",
    "    .load(r\"C:\\Users\\shravanr\\learning\\spark\\pyspark\\input_dir\\data_no_header.tsv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---+------+-----------+\n",
      "|EmployeeName|Department|Age|Salary| EmployeeId|\n",
      "+------------+----------+---+------+-----------+\n",
      "|       Smith|     Admin| 22|2500.0|          0|\n",
      "|       Jones|        HR| 30|4000.0|          1|\n",
      "|        John|        HR| 28|3000.0| 8589934592|\n",
      "|    Williams|     Admin| 23|2000.0|17179869184|\n",
      "|       Brown|     Admin| 25|3500.0|17179869185|\n",
      "+------------+----------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", \"HR\", 28, 3000.0),\n",
    "        (\"Smith\", \"Admin\", 22, 2500.0),\n",
    "        (\"Williams\", \"Admin\", 23, 2000.0),\n",
    "        (\"Jones\", \"HR\", 30, 4000.0),\n",
    "        (\"Brown\", \"Admin\", 25, 3500.0)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"EmployeeName\", \"Department\", \"Age\", \"Salary\"]).repartition(3)\n",
    "df = df.withColumn(\"EmployeeId\", F.monotonically_increasing_id()) #gives ids that are unique across partitions\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeName: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- EmployeeId: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeName: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- EmployeeId: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"Salary\", col(\"Salary\").cast(\"int\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_3_5_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
